{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cafcfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8b2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Easy21():\n",
    "    def __init__(self):\n",
    "        self.min_value, self.max_value = 1, 10\n",
    "        self.dealer_lowerbound = 16\n",
    "        self.hand_lowerbound, self.hand_upperbound = 0, 21\n",
    "    \n",
    "    def start_game(self):\n",
    "        return (np.random.randint(self.min_value, self.max_value+1),\n",
    "                np.random.randint(self.min_value, self.max_value+1))\n",
    "    \n",
    "    def reset_game(self):\n",
    "        return (np.random.randint(self.min_value, self.max_value+1),\n",
    "                np.random.randint(self.min_value, self.max_value+1))\n",
    "    \n",
    "    def draw(self):\n",
    "        card_value = np.random.randint(self.min_value, self.max_value+1)\n",
    "        if np.random.random_sample() <= 2/5:\n",
    "            return -card_value\n",
    "        else:\n",
    "            return card_value\n",
    "        \n",
    "    def step(self, player_value, dealer_value, action, terminate_turn):\n",
    "        if action == 0: # hit\n",
    "            player_value += self.draw()\n",
    "            if self.hand_lowerbound < player_value <= self.hand_upperbound:\n",
    "                reward = 0\n",
    "                terminate_turn = False\n",
    "            else:\n",
    "                reward = -1\n",
    "                terminate_turn = True\n",
    "        else: # stick\n",
    "            terminate_turn = True\n",
    "            while self.hand_lowerbound < dealer_value < self.dealer_lowerbound:\n",
    "                dealer_value += self.draw()\n",
    "            if not self.hand_lowerbound < dealer_value <= self.hand_upperbound:\n",
    "                reward = 1\n",
    "            elif player_value > dealer_value:\n",
    "                reward = 1\n",
    "            elif player_value < dealer_value:\n",
    "                reward = -1\n",
    "            else: # player_value == dealer_value\n",
    "                reward = 0\n",
    "        return player_value, dealer_value, reward, terminate_turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c7c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner():\n",
    "    def __init__(self, easy21, total_episodes, epsilon, gamma, alpha):\n",
    "        self.easy21 = easy21\n",
    "        self.qtable = np.zeros((22, 11, 2))\n",
    "        self.num_state_action = np.zeros((22, 11, 2))\n",
    "        self.num_state = lambda p, d: np.sum(self.num_state_action[p, d])\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.total_episodes = total_episodes\n",
    "        self.wins = 0\n",
    "    \n",
    "    def epsilon_greedy(self, player_value, dealer_value):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = random.randint(0, 1)\n",
    "        else:\n",
    "            action = np.argmax([self.qtable[player_value, dealer_value, action] for action in (0, 1)])\n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        episode_print_split = self.total_episodes // 5\n",
    "        for episode in range(total_episodes):\n",
    "            terminate_turn = False\n",
    "            state_action_reward = []\n",
    "            player_value, dealer_value = self.easy21.start_game()\n",
    "            \n",
    "            while terminate_turn == False:\n",
    "                action = self.epsilon_greedy(player_value, dealer_value)\n",
    "                self.num_state_action[player_value, dealer_value, action] += 1\n",
    "                player_value_new, dealer_value_new, reward, terminate_turn = self.easy21.step(player_value, dealer_value,\n",
    "                                                                                              action, terminate_turn)\n",
    "                state_action_reward.append([player_value, dealer_value, action, reward])\n",
    "                player_value, dealer_value = player_value_new, dealer_value_new\n",
    "            \n",
    "            self.easy21.reset_game()\n",
    "            \n",
    "            total_rewards = sum(sar[-1] for sar in state_action_reward)\n",
    "            \n",
    "            for (p, d, a, r) in state_action_reward:\n",
    "                self.qtable[p, d, a] += self.alpha * (self.gamma - self.qtable[p, d, a])\n",
    "            \n",
    "            if reward == 1:\n",
    "                self.wins += 1\n",
    "            if (episode + 1) % episode_print_split == 0:\n",
    "                print('training episode %i | win percentage %.2f'%(episode, self.wins / (episode + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0bcabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 1999 | win percentage 0.47\n",
      "training episode 3999 | win percentage 0.47\n",
      "training episode 5999 | win percentage 0.45\n",
      "training episode 7999 | win percentage 0.45\n",
      "training episode 9999 | win percentage 0.45\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 10000\n",
    "epsilon = 0.9\n",
    "gamma = 0.95\n",
    "alpha = 0.8\n",
    "\n",
    "easy21 = Easy21()\n",
    "q_learner = QLearner(easy21, total_episodes, epsilon, gamma, alpha)\n",
    "q_learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84cae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproxQLearner():\n",
    "    def __init__(self, easy21, total_episodes, epsilon, gamma, alpha):\n",
    "        self.easy21 = easy21\n",
    "        self.qtable = np.zeros((22, 11, 2))\n",
    "        self.num_state_action = np.zeros((22, 11, 2))\n",
    "        self.num_state = lambda p, d: np.sum(self.num_state_action[p, d])\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.total_episodes = total_episodes\n",
    "        self.wins = 0\n",
    "        self.intervals = {\n",
    "            'dealer': ((1, 4), (4, 7), (7, 10)),\n",
    "            'player': ((1, 6), (4, 9), (7, 12), (10, 15), (13, 18), (16, 21)),\n",
    "            'action': (0, 1)\n",
    "        }\n",
    "        self.feature_shape = tuple(len(self.intervals[key]) for key in ('dealer', 'player', 'action'))\n",
    "        self.weights = (np.random.rand(*self.feature_shape) - 0.5) * 0.001\n",
    "\n",
    "    def weights_function(self, player_value, dealer_value, action):\n",
    "        state_features = np.array([(d[0] <= dealer_value <= d[1]) and (p[0] <= player_value <= p[1])\n",
    "                             for d in self.intervals['dealer']\n",
    "                             for p in self.intervals['player']]).astype(int).reshape(self.feature_shape[:2])\n",
    "        features = np.zeros(self.feature_shape)\n",
    "        if action == 1:\n",
    "            features[:, :, 0] = state_features\n",
    "        return features.astype(int)\n",
    "    \n",
    "    def epsilon_greedy(self, player_value, dealer_value):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = random.randint(0, 1)\n",
    "            q_hat = np.sum(self.weights_function(player_value, dealer_value, action) * self.weights)\n",
    "        else:\n",
    "            q_hat, action = max(((np.sum(self.weights_function(player_value, dealer_value, a) * self.weights), a) for a in (0, 1)), key=lambda x: x[0])\n",
    "        return q_hat, action\n",
    "\n",
    "    def train(self):\n",
    "        episode_print_split = self.total_episodes // 5\n",
    "        #mean_reward = 0\n",
    "        for episode in range(total_episodes):\n",
    "            terminate_turn = False\n",
    "            state_action_reward = []\n",
    "            player_value, dealer_value = self.easy21.start_game()\n",
    "            dealer_first_value = dealer_value\n",
    "            weight_table = []\n",
    "           \n",
    "            while terminate_turn == False:\n",
    "                q_hat, action = self.epsilon_greedy(player_value, dealer_value)\n",
    "                self.num_state_action[player_value, dealer_value, action] += 1\n",
    "                player_value_new, dealer_value_new, reward, terminate_turn = self.easy21.step(player_value, dealer_value,\n",
    "                                                                                              action, terminate_turn)\n",
    "                state_action_reward.append([player_value, dealer_value, action, reward])\n",
    "                q_hat1, action1 = self.epsilon_greedy(player_value_new, dealer_value_new)\n",
    "                features = self.weights_function(player_value_new, dealer_value_new, action1)\n",
    "                delta_weights = self.alpha * (reward + self.gamma * q_hat1 - q_hat) * (self.gamma * self.weights + features)\n",
    "                self.weights += delta_weights\n",
    "                player_value, dealer_value = player_value_new, dealer_value_new\n",
    "            \n",
    "            self.easy21.reset_game()\n",
    "            \n",
    "            total_rewards = sum(sar[-1] for sar in state_action_reward)\n",
    "            \n",
    "            for (p, d, a, r) in state_action_reward:\n",
    "                self.qtable[p, d, a] += self.alpha * (self.gamma - self.qtable[p, d, a])\n",
    "\n",
    "            if reward == 1:\n",
    "                self.wins += 1\n",
    "            if (episode + 1) % episode_print_split == 0:\n",
    "                print('training episode %i | win percentage %.2f'%(episode, self.wins / (episode + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112e454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 1999 | win percentage 0.44\n",
      "training episode 3999 | win percentage 0.43\n",
      "training episode 5999 | win percentage 0.43\n",
      "training episode 7999 | win percentage 0.44\n",
      "training episode 9999 | win percentage 0.44\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 10000\n",
    "epsilon = 0.9\n",
    "gamma = 0.95\n",
    "alpha = 0.8\n",
    "\n",
    "easy21 = Easy21()\n",
    "approx_q_learner = ApproxQLearner(easy21, total_episodes, epsilon, gamma, alpha)\n",
    "approx_q_learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fd51a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
